7/26/2019
  I met with Gio and got the Pi camera. I was able to write a few scripts to take a still image
  and to show a live video feed until a kill key ('q') is pressed. Those scripts wont be uploaded
  until I get SSH working on my desktop. I tried for several hours to install TensorFlow onto the
  Pi but was unable to. I think that installing it from scratch will yield better results
  (wipe the pi and start over). I was able to get TF installed on my computer earlier this morning
  and ran a test cnn for the Mnist dataset on it. I am using Anaconda to manage my installs and versions.
  Still very new to Anaconda environments but figuring it out. Tomorrows goal is to put in eight hours
  of work and at minimum complete the project in ref 2. Ideally, I am able to finish the project in ref
  2 and then modify it to recognize cards in general. That would be enough to have a team try and put
  that model on the Pi and start working on movements.

7/27/2019
  11:00am
  Successfully installed TensorFlow GPU v1.10.0, cuDNN, and CUDA using ref 6.
  Ran hello world tensorflow version. Going to start with the tut from ref 2 now
  Anaconda env tfGPUTestP3.5
  11:40
  creating new Anaconda env that is a copy of tfGPUTestP3.5 called testGPUTF
  Added the following to TestGPUTF (ref 2)
  (tensorflow1) C:\> conda install -c anaconda protobuf
  (tensorflow1) C:\> pip install pillow
  (tensorflow1) C:\> pip install lxml
  (tensorflow1) C:\> pip install Cython
  (tensorflow1) C:\> pip install contextlib2
  (tensorflow1) C:\> pip install jupyter
  (tensorflow1) C:\> pip install matplotlib
  (tensorflow1) C:\> pip install pandas
  (tensorflow1) C:\> pip install opencv-python
  Tensorflow is still working in the new env

  List all conda envs
    conda info --envs

  Start env
    activate (env-name)

  Application to start
    Anaconda prompt

  testGPUTF

  # Name                    Version                   Build  Channel
_tflow_select             2.1.0                       gpu
absl-py                   0.4.1                    py35_0
astor                     0.7.1                    py35_0
attrs                     19.1.0                   pypi_0    pypi
backcall                  0.1.0                    pypi_0    pypi
blas                      1.0                         mkl
bleach                    3.1.0                    pypi_0    pypi
certifi                   2018.8.24                py35_1
colorama                  0.4.1                    pypi_0    pypi
contextlib2               0.5.5                    pypi_0    pypi
cudatoolkit               9.0                           1                     // Important
cudnn                     7.6.0                 cuda9.0_0
cycler                    0.10.0                   pypi_0    pypi
cython                    0.29.13                  pypi_0    pypi
decorator                 4.4.0                    pypi_0    pypi
defusedxml                0.6.0                    pypi_0    pypi
entrypoints               0.3                      pypi_0    pypi
gast                      0.2.0                    py35_0
grpcio                    1.12.1           py35h1a1b453_0
icc_rt                    2019.0.0             h0cc432a_1
intel-openmp              2019.4                      245
ipykernel                 5.1.1                    pypi_0    pypi
ipython                   7.7.0                    pypi_0    pypi
ipython-genutils          0.2.0                    pypi_0    pypi
ipywidgets                7.5.0                    pypi_0    pypi
jedi                      0.14.1                   pypi_0    pypi
jinja2                    2.10.1                   pypi_0    pypi
jsonschema                3.0.1                    pypi_0    pypi
jupyter                   1.0.0                    pypi_0    pypi
jupyter-client            5.3.1                    pypi_0    pypi
jupyter-console           6.0.0                    pypi_0    pypi
jupyter-core              4.5.0                    pypi_0    pypi
kiwisolver                1.1.0                    pypi_0    pypi
libprotobuf               3.6.0                h1a1b453_0
lxml                      4.4.0                    pypi_0    pypi
markdown                  2.6.11                   py35_0
markupsafe                1.1.1                    pypi_0    pypi
matplotlib                3.0.3                    pypi_0    pypi
mistune                   0.8.4                    pypi_0    pypi
mkl                       2018.0.3                      1
mkl_fft                   1.0.6            py35hdbbee80_0
mkl_random                1.0.1            py35h77b88f5_1
nbconvert                 5.5.0                    pypi_0    pypi
nbformat                  4.4.0                    pypi_0    pypi
notebook                  6.0.0                    pypi_0    pypi
numpy                     1.15.2           py35ha559c80_0
numpy-base                1.15.2           py35h8128ebf_0
opencv-python             4.1.0.25                 pypi_0    pypi
pandas                    0.25.0                   pypi_0    pypi
pandocfilters             1.4.2                    pypi_0    pypi
parso                     0.5.1                    pypi_0    pypi
pickleshare               0.7.5                    pypi_0    pypi
pillow                    6.1.0                    pypi_0    pypi
pip                       19.2.1                   pypi_0    pypi
prometheus-client         0.7.1                    pypi_0    pypi
prompt-toolkit            2.0.9                    pypi_0    pypi
protobuf                  3.6.0            py35he025d50_0
pygments                  2.4.2                    pypi_0    pypi
pyparsing                 2.4.1.1                  pypi_0    pypi
pyrsistent                0.15.3                   pypi_0    pypi
python                    3.5.6                he025d50_0
python-dateutil           2.8.0                    pypi_0    pypi
pytz                      2019.1                   pypi_0    pypi
pywinpty                  0.5.5                    pypi_0    pypi
pyzmq                     18.0.2                   pypi_0    pypi
qtconsole                 4.5.2                    pypi_0    pypi
send2trash                1.5.0                    pypi_0    pypi
setuptools                40.2.0                   py35_0
six                       1.11.0                   py35_1
tensorboard               1.10.0           py35he025d50_0
tensorflow                1.10.0          gpu_py35ha5d5ef7_0
tensorflow-base           1.10.0          gpu_py35h6e53903_0
tensorflow-gpu            1.10.0               hf154084_0
termcolor                 1.1.0                    py35_1
terminado                 0.8.2                    pypi_0    pypi
testpath                  0.4.2                    pypi_0    pypi
tornado                   6.0.3                    pypi_0    pypi
traitlets                 4.3.2                    pypi_0    pypi
vc                        14.1                 h0510ff6_4
vs2015_runtime            14.15.26706          h3a45250_4
wcwidth                   0.1.7                    pypi_0    pypi
webencodings              0.5.1                    pypi_0    pypi
werkzeug                  0.15.4                     py_0
wheel                     0.31.1                   py35_0
widgetsnbextension        3.5.0                    pypi_0    pypi
win-unicode-console       0.5                      pypi_0    pypi
wincertstore              0.2              py35hfebbdb8_0
zlib                      1.2.11               h62dcd97_3

12:15
  The test script object_detection_tutorial.ipynb works in the testGPUTF environment.
  launch with command jupyter notebook object_detection_tutorial.ipynb
  Finished with 15:23 of ref 2. Taking a break for lunch.

Left to do today:
  Find resolution of images in test picture see if they need to be the same (done)
  Find another image to test with and see the results. (done)
  -------
  Download and experiment with the masking software
  -------
  In general work towards completing ref 2.
  It would be awesome if I can complete ref 2 before dinner.

1:40
  Edited original test script to also identify a picture of a cell phone. It said it was a tv
  but thats okay at least we are able to give it images that don't fit a cookie cutter

  Less that 200kb each around 350 pictures for training
  less than 720x1280 resolution 20/80 split test/train

  Dont forget to re run this command every anaconda window
  set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim

2:30
  Ran into some problems with version compatibility back to where I was before lunch but with older verion

2:45
  After many trials I finally got the example from ref 2 training as expected. I also was able to bring up a
  tensorboard which is pretty cool. I will let the model train for an hour or so letting me take a break.

Next steps:
  I want to recreate the steps to get this training to run so that I know it wasnt a fluke
  I will just delete the trained files and try again.
  --- Projected for today --- ^
  After I do that and feel comfortable with the process, and document it, ill Finished
  the tut in ref 2 and make sure I can get it to work with a single image and then a web cam feed.
  After that I want to relabel those images with red/black (or just as card) labels rather than the values.

3:00
  I stopped training early to test out the object detection and I was able to get it to work on a
  single image! Which is awesome.

  Next step is to recreate what I did and feel more comfortable with the process.
  Just to test again I pulled a random image off of the internet and it worked with that too!
  new test image test2.JPG


  // SETUP!
  cd C:\tensorflow1\models\research\object_detection
  activate testGPUTF
  set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim
  // SETUP!

  Setup for tensorboard
  tensorboard --logdir=C:/tensorflow1/models/research/object_detection/training

  These three things need to be done to run anything

  To test on an image its pretty simple: python Object_detection_image.py
  To train - (Training actually continued from where it left off)
  python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
  python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssdlite_mobilenet_v2_coco.config
  python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_coco.config

  These three files need to be in the training folder to train from scratch
  faster_rcnn_inception_v2_pets.config (Model config file)
  labelmap.pbtxt (matching string labels to ids)
  pipeline.config (how many classes, where/how many images)

  To export the model:
  python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-XXXX --output_directory inference_graph
  python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-29993 --output_directory ssdmnv2_0052_inference_graph
  python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/XXXX.config --trained_checkpoint_prefix training/model.ckpt-xxx --output_directory XXX_inference_graph

  Next up, train one more model test it on another new image. Then hook up the webcam and try it there

4:20
  Successfully ran model with webcam to identify some playing cards. Will make note of process and upload
  the revised code and pointers as well.

  File is in ref2tut/webCamDetection.py

  use idle to open up the web cam script and then f5 to run it. Make sure no other application is using webcam.

4:40

  Final notes for now, today I started and finished the tut from ref 2. I got the webcam working and detecting cards
  however the model was not very accurate so I am going to let it train for a few hours until later tonight and
  try again. I am excited by this progress the next step is to relabel all of the cards as just 'card' rather than
  'jack' 'king' and such. This will then setup for us to start with a 'card pickeruper' and then move into retraining
  for poop. The biggest next hassle will be to make sure that the model we train on the computer is suitable and
  compatible with the Pi. Reference 1 has some great information in that area once we get the robot trained for just cards.

  In the mean time, teams can take pictures of playdough poop and start labeling them with labelimg, starting next week
  they should also be able to attempt to put tensorflow and out model on the Pi.

  Next session goal: Same thing but just identifying them as cards (Maybe even a dot at the center)

10:30pm
  Working on relabeling all of the images to 'card'

11:30pm
  Wrote an awesome script to do this all for me in python check it out in cardcv
  Replaced all of the ids for the cards as just card not a rank.
  Sadly this is only useful once but still amazing because it saves over a day of work

  Going to check out the newest network and try it out on the web cam one more time
  because its fun but mostly because I want to make sure I can still do that

11:45
  The model def still works better than before but not 100% but thats okay because all we
  care about is detection and not classification.

  Going to try and train on the newest dataset

12:00
  Important to remember that you will need to change a some things in a few files
  when classifying a different number of things.

  The new model (with one class) is training now

  Also: This is no longer useful because the current network is detecting AND classifying
        but we only need to detect. This is not Important now but when it goes on the pi
        it will be.

  Assuming this works well we can then find the center of the card then we are ready to find
  a model that can fit on the pi.

1:00am
  Finishing for tonight. Today I finished a net that detects cards with a webcam feed.
  Goal for tomorrow: with the limited time that I have I would like to attempt and install
  tensorflow on the pi again. Also check into if the net I currently have is compatible with the Pi

  I just realized that I have been training using tensorflow GPU and I don't know if that will conflict
  when it is on the pi. If so, ill train using tensorflow and it shouldn't be a big deal.

7/28/19
10:00pm
  There was a meeting today and I will transcribe some notes later. Right now I am trying to install tf on the pi
  I was able to get miniConda on it and trying to get a python3.5 envs

  Successfully created an env named py35 with the correct version of python installed.
  Will now try and hopefully, but its unlikely, to do an install of tf and python 3.5 just like on the pc

  To activate the env on mini conda the command is
    source activate {name}

  Successfully installed TF on the pi in the Anaconda env called py35tf

11:30pm
  Currently making the protobuf lib eta 3 hours
  Watching the rest of his video it looks like he is able to transfer any model onto the pi
  which is great, he also has another video to get the location of a detected object.
  I think I can figure that our myself but having that will be helpful.
  I am hopeful that I can finish this by tomorrow.

  An issue that I see in the future is that I am installing the latest version of tf/protobuf
  because thats what I got to work so we will see how that plays out in the future
  I am also very excited to see how easy anaconda is to work with.

  Today I met with some menbers of the team (Austin, Gio, and Eric) and discussed a plan/goals
  before the start of the semester. We want to low ball it so that way we can be sure to hit them.

  Goal for CS:
    We want to get any object detection running on the Pi, preferably with a model we trained on tensorflow-gpu
    on one of our computers. If we can do that then we can stop worrying out 'if' the pi can do it and start
    working on creating a database of images. Realistically this can happen as soon as I finish ref 1.

  Goal for ME:
    The ME team wants to have a robot moving around the room without bumping into walls all on internal power.
    Gio seems to be pretty confident that they can do this before im back from break on Aug 5.

  Tasks for now:
    Gio:
      Continue working on actually making the robot (other ME things that are over my head)
    Austin and Eric:
      We talked about how we are going to have multiple scripts running on the pi at one time
      They are going to see if they can get two scripts talking to each other while running
    Me:
      I am working on getting tensorflow installed on the pi (done) and to complete reference 1

  We talked about how the mult different scripts are going to interact with each other.
  The goal is to not have everything in one script to avoid a 'choppy' looking robot.
  After Austin and Eric figure out how two python scripts can talk to each other we can
  make a better plan on how to most efficiently do this. Also it doesnt matter if they all
  dont work individually.

  make check command 7:56 in ref 1

7/29/2019
7:30pm
  Starting work on comms between two python script.

  Create new anaconda env
    conda create --name myenv

8/4/2019
10:00am
  Working on getting comms working with key presses. The goal is to get a "triangle" like comm system that
  can be expanded with more nodes.

12:20pm
  I got the comms to work from a top down parent program on a key press. so A talks to b and c when a key is pressed
  not totally there yes and not super clean code but its progress.

I worked alot on comms that day and met with Austin to try and get CV on the pi without any luck

8/7/2019
12:00
  Started working on comms again. Im pretty sure Ive got the structure together now just to write the function

1:10
  Message structure is as follows
  SenderReciverMessage#
  eg.
  001010Message#

  000 parent
  001 sctipt A (prob TF)
  010 ...
  ...

  1:40
  Able to send messages from one child to another through the parent
  Next is to try and get the parent listen to multiple children

  2:00
  made great progress on the comms side of things. Still working on getting
  the parent to listen to two children at the same time. Should just be
  looping trough an array. Breaking for lunch

  3:15
    Starting work on comms again

  4:20
    Picked up Pi from Austin
    Ill work on that tomorrow.
    Im going to upload the code I have now to github
    for the comms so he can check it out

8/8/2019
  5:00

  9:00pm
  Change python version
  check the version of python: ls /usr/bin/python*
  alias: alias python='/usr/bin/pythonxx'
  re-login: . ~/.bashrc
  check the python version again: python --version

8/9/2019
7:00am
  Updates from yesterday and the day before.
  We started experimenting with comms on the Pi and ran into issues with
  the scheduler. I think this can be resolved by just installing the lib
  Successfully but that has proven to be difficult. Austin is also working on
  a version of the comms without using a scheduler. I am going to keep working
  on a version with a scheduler. Today I plan on minimizing the number of
  os calls like os open and close.
  Updates from CV we got cv2 and tensorflow working and the Object detection
  working. It ran at the 1-1.25 frames a second that we thought it would.
  Good news: we are able to transfer any model we train on a computer onto the
  pi with just moving a few files. Bad news: the model that I trained using TF
  on my computer is way too bulky for the PI and runs at like 0.06 frames
  and thats unexceptable. I think that this can be resolved by choosing a
  different model from the model farm. I dont want to do any of that today
  seeing as that will be a hefty process.

  Going to work on comms again right now, thats more enjoyable

9:00am
  Stopping after working on comms, there is a problem with os open
  but major problem resolved of having too many files opened at one time
  correctly closing files now

12:00pm
  Starting working on the new tensorflow model.

2:00pm
  I have the model training right now.
  ssd_mobilenet_v1_coco_2018_01_28
  It is def slower than the other one but that is to be expected.
  On ref 1 or 2 he trains it for 8 hours. I stop it once I am back
  from tonights meetings.
  steps are taking about 1.3 seconds. as compared to about 0.1 something on
  the other model. This one should work much better on the Pi but it is
  obvious that we will need to learn much more about tensorflow to create
  a better network.

  I think we should keep our expectations low. Only go for on concrete set size/color

  Once we get things working during the semester we need to have one team member only working
  on this problem. I would like to do that.

4:30pm
  After doing some research I think we should try training on
  ssdlite_mobilenet_v2_coco
  it is in the model zoo
  https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md
  Deved in april and this is the paper with it
  https://arxiv.org/pdf/1801.04381.pdf

10:25pm
  Met with others today, updates to be typed out Tomorrow
  mobilenet_ssd_v2_coco_quant_postprocess_edgetpu

8/10/2019
9:00am
  Let the model train overnight just to refresh I am training on
  ssd_mobilenet_v1_coco_2018_01_28
  Since then I have found many other candidates for the base net
  1)  ssd_mobilenet_v2_coco
  2)  ssdlite_mobilenet_v2_coco
  3)  mobilenet_ssd_v2_coco_quant_postprocess_edgetpu
  I am going to export the model and test it on the pi
  mostly looking for a target fps rather an accuracy
  Also a few things to note that number 3 on that list
  has done up to 30 fps which is amazing.
  There is also a bit of hardware sold by google
  that is said to speed up the pi its like $125
  So we will have to look into that.
  With limited time today my goal is to get that network exported
  and on the pi and then start training the next network

  Export Successful:
  python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v1_coco.config --trained_checkpoint_prefix training/model.ckpt-56327 --output_directory ssd_mobilenet_v1_coco_inference_graph

  Files needed for transfer to the Pi:
    "model.zip"
      model/
      model/checkpoint
      model/frozen_inference_graph.pb
      model/model.ckpt.data-00000-of-00001
      model/model.ckpt.index
      model/model.ckpt.meta
      model/saved_model/
      model/saved_model/saved_model.pb
      model/saved_model/variables/
      label_map.pbtxt

  Later today, throw it on the Pi, check fps/accuracy mostly caring about fps
  and then start training the next model

1:00pm
  Working on getting that model on the pi and testing FPS

3:00
  I was able to get the model up and running on the Pi which is awesome and it
  was running at 1-1.3 fps which is the target amount.

  Working on starting to train
  ssd_mobilenet_v2 now

  Starting to train ssd_mobilenet_v2 right now. Going to upload
  all of the models and the configs as I can.

  We are ordering the pi4 that should make the pi faster it also might allow
  us to get that "36 fps" on reference 11

  I want to train ssdlite_mobilenet_v2_coco next

9:30
  The model seems to have similar performance to the v1. We will have to
  see the difference on the Pi. I am going to stop it about at the time area
  around 56k steps (Or whatever it is when I wake up)

2:10am
  Made tons of progress on object tracking
  I think we are really close to having a Unique id system
  Think will make picking up with multi objects possible
  I dont know how well it works on the Pi yet...
  I will spend tomorrow commenting the detection scripts

8/11/2019
3:00pm
  Going to export the model I trained last night as put it on the Pi

4:10pm
  After a little testing it appears this network is better than the last
  both still suffer from detection from far away just I suspect thats the
  dataset problem not the network problem

  It also appeared that the network was on average faster maintaining a 1.2-1.3
  fps

  I am going to start training the next model
  ssdlite_mobilenet_v2_coco
  and see how it compares

6:00pm
  Breaking for dinner, going to regroup before the next week.
  Id like to have a break down of things to get done before the
  semester begins

  Im slightly nervous that the config files for the past 2 models have been
  wrong, I dont think thats the case but I can confirm after this model is
  done training.

  I want to start getting pictures as soon as possible, mapping out the field of
  view and everything.

8:45pm
  Going to start working on a plan for this week and the next
  14 days until the semester starts.

9:40
  From what Ive read it looks like it doesnt matter what we take the pictures from
  but we will need to resize them based on the model we will be using.
  For that reason I think we might want to wait till we have the pi4 and try
  each of the models before labeling the imgaes. I saw a few scripts that relabeled
  image after crops though so it shouldn't be too hard.

Initial Dataset Collection Plan
  1) For every terrain (background) (3) {1350}
    2) For every poop object (3) {450}
      3) For every vertical FOV zone (Size zone) (5) {150}
        4) For every horizontal FOV zone (angle zone) (10-50)
          5) Take a picture (1)

After we know what the field of view the camera will have we can revise this plan

The ssdlit model is training now and will be done in the morning.
Tomorrow I would like to try and get the fov figured out if Gio is free

As far as taking the photos it would be awesome if we had the ME team create what
we have now for the picam but for an iphone. Worst case is that we take all of
the photos from the pi and its just a really long, but effective process

I heard from Austin that he got linux on the pi, I think we should stick with
raspian but if we can use standard linux without losing any preformance then
thats the prefered option. I am going to upload a few links to the Database repo
later tonight

Plans for the week of 8/12/2019
  1) Become more comfortable with tf, understand more of whats going on before
      I get the frozen 'fine tuning' model
  2) Get the FOV of the robot down and start taking picture/label/processing them
  3) Comms, try and get another script running on the Pi to just spit out the instructions
     of what to do. This should be enough to tell us if we need to use more than 1 pi

  Summary:
    1) Learn more about TF
    2) Get FOV of camera
    3) Get two scripts running on the Pi

8/12/2019
1:00pm
  Finished training the ssdlite model, havent put it on the pi yet but it looks to be more
  accurate than the ssd_mobilenet_v2

  I bought some foam and put it on the stand for the pi. Hoping to get FoV this week

  Finished setting up the stand and ported the ssdlite model onto the pi
  it was again faster a steady 1.3 fps but it wasn't very accurate to cards on the table,
  I am hoping that its because of the images its trained on.

4:00pm
  Tested all of the networks on 4 cards.

  ssd_mobilenet_v1_coco_2018
    Same speed as v2 but less accuracy
  ssd_mobilenet_v2_coco_2018
    Reasonable speed 1-1.2 frames a second
    detected 2/4 cards on table
  ssdlite
    Fastest 1.3 on average
    detected 0/4 cards on table
  card (Ressnet)
    slowest 0.06 FPS
    detected all 4 of the cards

5:30pm
  Awesome update
  with ssd v2 I was able to get all 4 cards detected, with a fp here and There
  This only came after removing the plastic infront of the camera that was
  obstruting or at least altering the view.

  I want to re run the trials with the other models it still is nowhere close to
  the 99% acc with resnet but thats just not practal.

  I am excited to see how much impact the rpi4 has and if we can use tflit/google coral

  I have left the ssd v2 running for quite some time and it doesnt seem to be over heating
  and there is no fan. I still think we should use one just in case.

  Exciting times

  Things to do next:
    Fifo comms with the detection script

8/13/2019
1:00pm
  Going to work on creating two scripts
  one for taking pictures and storing them
  with names for the database

  Another for simulating the fifos

3:45pm
  It looks like I was able to get a anotation sctipt working but
  when it installed it messed up something with the tracking lib
  thats okay because tacking isnt a real issue yet. Next time i
  install somthing I am going to create a new env so that doesnt
  happen again.

  Next step is to get that to work on the card data base and understand
  that init script better. This way we can turn our database into many pictures
  and save some time.

5:35pm
  the picture script now saves to a ubs drive
  going to try and run that script on the card dataset
  and train on those. See if there is a difference

6:30pm
  Need to create a script for cleaning the resized image/xml files

  python generate_tfrecord.py --csv_input=reducedImages\train_labels.csv --image_dir=reducedImages\train --output_path=reducedTrain.record
  python generate_tfrecord.py --csv_input=reducedImages\test_labels.csv --image_dir=reducedImages\test --output_path=reducedTest.record

8:30pm
  Finally got the new model training started around 7pm this one is training on the same images as the others
  but with cops to 300x300 this will hopefully make the network more accurate because there will not be any
  squishing of data

8/14/2019
12:45pm
  The model ssd_mobilenet_v2 that is currently being trained on the new dataset of croped 300x300 photos
  is still training and looks to have good results. I am going to let this one train for longer than the others
  Finally got tensorbard up and running.

  Today id actaully like to work on the comms or FoV
  Tomorrow I can go back to data augmentation. I was thinking about not only doing crops of 300x300
  but also crops of other sizes.

2:20pm
  Finished writing script to talk with the movement script. right now movement does not talk back to OD but that is less Important

3:00pm
  Created a title template and adding Documentation

8/24/2019
8:00am
  After a nice hiatus I met with the team yesterday to go over a plan for when the semester starts.
  This morning we are going to try and get the OD script to work with the motors.

2:45pm
  Got the motors and the relay to work with the OD script. Going to comment everything now

8/25/2019
9:30
  This morning Im going to try and write the script to not have to have that communication
  one master script. We can test this out on monday

8/26/2019
8:00am
  going to continue working on the contrast script i started yesterday. Meeting with gio and austin at 9

3:00pm
  Got back from class, going to finish resume and start reading git
  Working on video contrast
  We decided we only need one script. So thats awesome. Need to update OG script with new code

8/29/2019
Things to get done this week?
  Copy of SD card
  Get Buster lite installed on an sd card
  Get tensorflow lite installed on that same sd
  Ba able to transfer models from normal tf to tf lite
  Get tensorflow lite to work with models we make on my computer
  Get field of view of camera
  Figure out how many pictures and of what angles we need
  Start taking those pictures
  Figure out how to start a program in headless mode (ie with start button)
  Border bounding box
  Remote control
  Abstract code out of one file
  Video feed through ssh
  Video/monitor using HIDM

9/6/2019
Creating a new conda env to update tf for tflite on computer -> tf090619
Creating a new conda env for tflite -> tflite090619

https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py
https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tpu_exporters.md
https://medium.com/datadriveninvestor/mobile-object-detector-with-tensorflow-lite-9e2c278922d0

// WORKS
python object_detection/export_tflite_ssd_graph.py --pipeline_config_path C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_coco_inference_graph/pipeline.config --trained_checkpoint_prefix C:/tensorflow1/models/research/object_detection/ssd_mobilenet_v2_coco_inference_graph/model.ckpt --output_directory C:/tensorflow1/models/research/object_detection/testTFlite

https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193


bazel run -c opt tensorflow/lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops

bazel.exe run -c opt lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops

bazel-0.26.1-windows-x86_64.exe run -c opt lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops

bazel run --config=opt lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops

bazel-0.26.1-windows-x86_64.exe run --config=opt lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops

bazel-0.26.1-windows-x86_64.exe run --config=opt tensorflow/lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops


bazel run -c opt tensoflow/lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops


bazel run -c opt tensoflow/lite/toco:toco -- --input_file=D:/Computer-Vision/ODScripts/conversion/tflite_graph.pb --output_file=D:/Computer-Vision/ODScripts/conversion/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops

tflite_convert --output_file=D:/Computer-Vision/ODScripts/conversion/foo.tflite --saved_model_dir=D:/Computer-Vision/ODScripts/conversion

https://medium.com/datadriveninvestor/mobile-object-detector-with-tensorflow-lite-9e2c278922d0

9/17/2019
Training model with poop pictures
TENSOBOARD:
  tensorboard --logdir=C:/tensorflow1/models/research/object_detection/training --host localhost --port 8088

New anaconda envs

TRAINING POOP MODEL
cd C:\tensorflow1\models\research\object_detection
activate tf_gpu917
set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim
python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_mobilenet_v2_coco.config
TRAINING POOP MODEL

9/18/2019

To restart training:

edit checkpoint file
  model_checkpoint_path: "model.ckpt-29993"
  all_model_checkpoint_paths: "model.ckpt-29993"

have .ckpt-xxx.index .ckpt-xxx.data .ckpt-xxx.meta all there

D:\Models\poop\inference_graphs

EXPORTING TO DIR
python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/XXX.config --trained_checkpoint_prefix training/model.ckpt-XXX --output_directory D:\Models\poop\inference_graphs\XXX
python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-28496 --output_directory D:\Models\poop\inference_graphs\faster_rcnn_inception_v2_pets_v1
python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-70967 --output_directory D:\Models\poop\inference_graphs\ssd_mobilenet_v2_0037_v2
python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-94237 --output_directory D:\Models\poop\inference_graphs\ssd_mobilenet_v2_0037_v3
python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/ssd_mobilenet_v2_coco.config --trained_checkpoint_prefix training/model.ckpt-108679 --output_directory D:\Models\poop\inference_graphs\ssd_mobilenet_v2_v4

tflite_convert --output_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/optimized_graph.tflite --graph_def_file=/home/wang/Downloads/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb --inference_type=FLOAT --inference_input_type=QUANTIZED_UINT8 --input_arrays=ImageTensor --input_shapes=1,513,513,3 --output_arrays=SemanticPredictions –allow_custom_op
